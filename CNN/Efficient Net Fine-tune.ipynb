{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"NzHP2rndF-0G"},"source":["### import"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"YD7vyST9GdHv"},"outputs":[],"source":["import numpy as np\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch import optim\n","from torch_optimizer import RAdam\n","from torch_optimizer import AdamP\n","from torchvision.datasets import FashionMNIST\n","from torchvision import transforms\n","import wandb\n","import os\n","import sys\n","from omegaconf import OmegaConf # configuration 관리\n","from omegaconf import DictConfig\n","\n","from efficientnet_pytorch import EfficientNet"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["project_root = \"C:/Users/rlduf/Desktop/Github/DeepLearning\"\n","sys.path.append(project_root)\n","data_root = os.path.join(os.getcwd(), \"data\")"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"QX3soatAMk5B"},"outputs":[],"source":["from data_utils import dataset_split"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"lISGUqfUG_Ny"},"outputs":[],"source":["transform = transforms.Compose(\n","    [\n","        transforms.Resize(224),\n","        transforms.ToTensor(),\n","        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n","    ]\n",")\n","\n","fashion_mnist_dataset = FashionMNIST(data_root, download=True, train=True, transform=transform)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0waoPp23NcE0"},"source":["### dataloader"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"A5Nq1tZiJ84g"},"outputs":[],"source":["datasets = dataset_split(fashion_mnist_dataset, split=[0.9, 0.1])\n","\n","train_dataset = datasets[\"train\"]\n","val_dataset = datasets[\"val\"]\n","\n","train_batch_size = 100\n","val_batch_size = 10\n","\n","train_dataloader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=1\n",")\n","val_dataloader = torch.utils.data.DataLoader(\n","    val_dataset, batch_size=val_batch_size, shuffle=False, num_workers=1\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Lwd5nW_3N37z"},"source":["### model\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"42in1xpAqQDY"},"source":["### CNN"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"dCEx_VgEqTiB"},"outputs":[{"name":"stdout","output_type":"stream","text":["layer_1:\n","  conv2d_in_channels: 1\n","  conv2d_out_channels: 32\n","  conv2d_kernel_size: 3\n","  conv2d_padding: 1\n","  maxpool2d_kernel_size: 2\n","  maxpool2d_stride: 2\n","layer_2:\n","  conv2d_in_channels: 32\n","  conv2d_out_channels: 64\n","  conv2d_kernel_size: 3\n","  conv2d_padding: 0\n","  maxpool2d_kernel_size: 2\n","  maxpool2d_stride: 1\n","fc_1:\n","  in_features: 2304\n","  out_features: 512\n","fc_2:\n","  in_features: 512\n","  out_features: 128\n","fc_3:\n","  in_features: 128\n","  out_features: 10\n","dropout_prob: 0.25\n","\n"]}],"source":["_cnn_cfg_dict: dict = {\n","    \"layer_1\": {\n","        \"conv2d_in_channels\": 1,\n","        \"conv2d_out_channels\": 32,\n","        \"conv2d_kernel_size\": 3,\n","        \"conv2d_padding\": 1,\n","        \"maxpool2d_kernel_size\": 2,\n","        \"maxpool2d_stride\": 2,\n","    },\n","    \"layer_2\": {\n","        \"conv2d_in_channels\": 32,\n","        \"conv2d_out_channels\": 64,\n","        \"conv2d_kernel_size\": 3,\n","        \"conv2d_padding\": 0,\n","        \"maxpool2d_kernel_size\": 2,\n","        \"maxpool2d_stride\": 1,\n","    },\n","    \"fc_1\": {\n","        \"in_features\": 2304, #  수정 필요!\n","        \"out_features\": 512,\n","    },\n","    \"fc_2\": {\n","        \"in_features\": 512,\n","        \"out_features\": 128,        \n","    },\n","    \"fc_3\": {\n","        \"in_features\": 128,\n","        \"out_features\": 10,\n","    },\n","    \"dropout_prob\": 0.25,\n","}\n","_cnn_cfg = OmegaConf.create(_cnn_cfg_dict)\n","print(OmegaConf.to_yaml(_cnn_cfg))\n","\n","class CNN(nn.Module):\n","    def __init__(self, cfg: DictConfig = _cnn_cfg):\n","        super().__init__()\n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(\n","                in_channels=cfg.layer_1.conv2d_in_channels,\n","                out_channels=cfg.layer_1.conv2d_out_channels,\n","                kernel_size=cfg.layer_1.conv2d_kernel_size,\n","                padding=cfg.layer_1.conv2d_padding\n","            ),\n","            nn.BatchNorm2d(cfg.layer_1.conv2d_out_channels),\n","            nn.ReLU(),\n","            nn.MaxPool2d(\n","                kernel_size=cfg.layer_1.maxpool2d_kernel_size,\n","                stride=cfg.layer_1.maxpool2d_kernel_size\n","            )\n","        )\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(\n","                in_channels=cfg.layer_2.conv2d_in_channels,\n","                out_channels=cfg.layer_2.conv2d_out_channels,\n","                kernel_size=cfg.layer_2.conv2d_kernel_size,\n","                padding=cfg.layer_2.conv2d_padding\n","            ),\n","            nn.BatchNorm2d(cfg.layer_2.conv2d_out_channels),\n","            nn.ReLU(),\n","            nn.MaxPool2d(\n","                kernel_size=cfg.layer_2.maxpool2d_kernel_size,\n","                stride=cfg.layer_2.maxpool2d_kernel_size\n","            )\n","        )\n","        self.fc1 = nn.Linear(\n","            in_features=cfg.fc_1.in_features,\n","            out_features=cfg.fc_1.out_features,\n","        )\n","        self.fc2 = nn.Linear(\n","            in_features=cfg.fc_2.in_features,\n","            out_features=cfg.fc_2.out_features,\n","        )\n","        self.fc3 = nn.Linear(\n","            in_features=cfg.fc_3.in_features,\n","            out_features=cfg.fc_3.out_features,\n","        )\n","        self.dropout = nn.Dropout2d(cfg.dropout_prob)\n","\n","\n","    def forward(self, x):\n","        out = self.layer1(x)\n","        out = self.layer2(out)\n","        out = out.view(out.size(0), -1)\n","        out = self.fc1(out)\n","        out = self.dropout(out)\n","        out = self.fc2(out)\n","        out = self.fc3(out)\n","        return out\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"HVNk_qol3eCz"},"outputs":[{"name":"stdout","output_type":"stream","text":["efficient_net_model_name: efficientnet-b1\n","num_classes: 10\n","\n"]}],"source":["_efficient_finetune_cfg_dict: dict = {\n","    \"efficient_net_model_name\": \"efficientnet-b1\",\n","    \"num_classes\": 10\n","}\n","_efficient_finetune_cfg_cfg = OmegaConf.create(_efficient_finetune_cfg_dict)\n","print(OmegaConf.to_yaml(_efficient_finetune_cfg_cfg))\n","\n","class EfficientNetFinetune(nn.Module):\n","    def __init__(self, cfg: DictConfig = _efficient_finetune_cfg_cfg):\n","        super().__init__()\n","        self.efficientnet = EfficientNet.from_pretrained(\n","            cfg.efficient_net_model_name,\n","            cfg.num_classes\n","        )\n","        self.efficientnet\n","    \n","    def forward(self, x):\n","        out = self.efficientnet(x)\n","        return out"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"tMM1y75wql8f"},"source":["### Learning Rate Scheduler \n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"3y1m5T3-qqCk"},"outputs":[],"source":["# Warmup Scheduler\n","class WarmupLR(optim.lr_scheduler.LambdaLR):\n","\n","    def __init__(\n","        self,\n","        optimizer: optim.Optimizer,\n","        warmup_end_steps: int,\n","        last_epoch: int = -1,\n","    ):\n","        \n","        def wramup_fn(step: int):\n","            if step < warmup_end_steps:\n","                return float(step) / float(max(warmup_end_steps, 1))\n","            return 1.0\n","        \n","        super().__init__(optimizer, wramup_fn, last_epoch)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"b8tljBSaQBZB"},"source":["### model, optimizer, loss function, logger"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z8p_hsjsO-on"},"outputs":[],"source":["# define model.\n","# model = CNN(cfg=_cnn_cfg)\n","model = EfficientNetFinetune(cfg=_efficient_finetune_cfg_cfg).to(device)\n","model_name = type(model).__name__\n","\n","# define loss\n","loss_function = nn.CrossEntropyLoss()\n","\n","# define optimizer\n","lr = 1e-3\n","# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n","optimizer = RAdam(model.parameters(), lr=lr)\n","# optimizer = AdamP(model.parameters(), lr=lr)\n","optimizer_name = type(optimizer).__name__\n","\n","# define scheduler\n","scheduler = None\n","# scheduler = WarmupLR(optimizer, 1500)\n","scheduler_name = type(scheduler).__name__ if scheduler is not None else \"no\"\n","\n","max_epoch = 30\n","\n","# define model path\n","log_dir = f\"{model_name}-{optimizer_name}-{lr}-{scheduler_name}\"\n","log_model_path = os.path.join(log_dir, 'models')\n","os.makedirs(log_model_path, exist_ok=True)\n","\n","# define wandb\n","project_name = \"fashion_mnist\"\n","run_tags = [project_name]\n","wandb.init(\n","    project=project_name,\n","    name=log_dir,\n","    tags=run_tags,\n","    config={\"lr\": lr, \"model_name\": model_name, \"optimizer_name\": optimizer_name, \"scheduler_name\": scheduler_name},\n","    reinit=True,\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lnGxA90KyyUi"},"source":["### Early Stopping callback Object Class"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"WyuzST93ynLE"},"outputs":[],"source":["# With some modifications, source is from https://github.com/Bjarten/early-stopping-pytorch\n","\n","class EarlyStopping:\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.ckpt', trace_func=print):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved.\n","                            Default: 7\n","            verbose (bool): If True, prints a message for each validation loss improvement. \n","                            Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","                            Default: 0\n","            path (str): Path for the checkpoint to be saved to.\n","                            Default: 'checkpoint.ckpt'\n","            trace_func (function): trace print function.\n","                            Default: print            \n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","        self.path = path\n","        self.trace_func = trace_func\n","\n","    def __call__(self, val_loss, model):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        '''Saves model when validation loss decrease.'''\n","        if self.verbose:\n","            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        \n","        filename = self.path.split('/')[-1]\n","        save_dir = os.path.dirname(self.path)\n","        torch.save(model, os.path.join(save_dir, f\"val_loss-{val_loss}-{filename}\"))\n","        self.val_loss_min = val_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oq-XpcGXQu1v"},"outputs":[],"source":["# define EarlyStopping.\n","early_stopper = EarlyStopping(\n","    patience=3, verbose=True, path=os.path.join(log_model_path, \"model.ckpt\")\n",")\n","\n","# do train with validation.\n","train_step = 0\n","for epoch in range(1, max_epoch+1):\n","    # valid step\n","    with torch.no_grad():\n","        val_loss = 0.0\n","        val_corrects = 0\n","        model.eval()\n","\n","        for val_batch_idx, (val_images, val_labels) in enumerate(\n","            tqdm(val_dataloader, position=0, leave=True, desc=\"validation\")\n","        ):\n","            val_images, val_labels = val_images.to(device), val_labels.to(device)\n","\n","            # forward\n","            val_outputs = model(val_images)\n","            _, val_preds = torch.max(val_outputs, 1)\n","            \n","            # loss & acc\n","            val_loss += loss_function(val_outputs, val_labels) / val_outputs.shape[0]\n","            val_corrects += torch.sum(val_preds == val_labels.data) / val_outputs.shape[0]\n","    \n","    # valid step logging\n","    val_epoch_loss = val_loss / len(val_dataloader)\n","    val_epoch_acc = val_corrects / len(val_dataloader)\n","    \n","    print(\n","        f\"{epoch} epoch, {train_step} step: val_loss: {val_epoch_loss}, val_acc: {val_epoch_acc}\"\n","    )\n","\n","    # wandb log\n","    wandb.log({\n","        \"Loss/val\": val_epoch_loss,\n","        \"Acc/val\": val_epoch_acc,\n","        \"Images/val\": wandb.Image(val_images),\n","        \"Outputs/val\": wandb.Histogram(val_outputs.detach().cpu().numpy()),\n","        \"Preds/val\": wandb.Histogram(val_preds.detach().cpu().numpy()),\n","        \"Labels/val\": wandb.Histogram(val_labels.data.detach().cpu().numpy()),\n","    }, step=train_step)\n","\n","    # check model early stopping point & save model if the model reached the best performance.\n","    early_stopper(val_epoch_loss, model)\n","    if early_stopper.early_stop:\n","        break\n","    \n","    # train step\n","    current_loss = 0\n","    current_corrects = 0\n","    model.train()\n","\n","    for batch_idx, (images, labels) in enumerate(\n","         tqdm(train_dataloader, position=0, leave=True, desc=\"training\")\n","    ):\n","        images, labels = images.to(device), labels.to(device)\n","\n","        current_loss = 0.0\n","        current_corrects = 0\n","\n","        # Forward\n","        # get predictions\n","        outputs = model(images)\n","        _, preds = torch.max(outputs, 1)\n","        \n","        # get loss (Loss 계산)\n","        loss = loss_function(outputs, labels)\n","\n","        # Backpropagation\n","        # optimizer 초기화 (zero화)\n","        optimizer.zero_grad()\n","\n","        # Perform backward pass\n","        loss.backward()\n","\n","        # Perform Optimization\n","        optimizer.step()\n","\n","        # Perform LR scheduler Work\n","        if scheduler is not None:\n","            scheduler.step()\n","\n","        current_loss += loss.item()\n","        current_corrects += torch.sum(preds == labels.data)\n","\n","        if train_step % 100 == 0:\n","            train_loss = current_loss / 100\n","            train_acc = current_corrects / 100\n","\n","            print(\n","                f\"{train_step}: train_loss: {train_loss}, train_acc: {train_acc}\"\n","            )\n","\n","            # wandb log\n","            wandb.log({\n","                \"Loss/train\": train_loss,\n","                \"Acc/train\": train_acc,\n","                \"Images/train\": wandb.Image(images),\n","                \"Outputs/train\": wandb.Histogram(outputs.detach().cpu().numpy()),\n","                \"Preds/train\": wandb.Histogram(preds.detach().cpu().numpy()),\n","                \"Labels/train\": wandb.Histogram(labels.data.detach().cpu().numpy()),\n","                \"Learning Rate\": scheduler.get_last_lr()[0]\n","            }, step=train_step)\n","\n","            current_loss = 0\n","            current_corrects = 0\n","\n","        train_step += 1\n","        \n","    print('\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ymnDN1VkYOvt"},"outputs":[],"source":["# load model\n","model_path = 'model.ckpt'\n","loaded_model = torch.load(os.path.join(log_model_path, model_path)).to(device)\n","loaded_model.eval()\n","print(loaded_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AHvB9xAnYWBk"},"outputs":[],"source":["def softmax(x, axis=0):\n","    \"numpy softmax\"\n","    max = np.max(x, axis=axis, keepdims=True)\n","    e_x = np.exp(x - max)\n","    sum = np.sum(e_x, axis=axis, keepdims=True)\n","    f_x = e_x / sum\n","    return f_x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_UoTf2ztY7nT"},"outputs":[],"source":["test_batch_size = 128\n","test_dataset = FashionMNIST(data_root, download=True, train=False, transform=transform)\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=1)\n","\n","test_labels_list = []\n","test_preds_list = []\n","test_outputs_list = []\n","for i, (test_images, test_labels) in enumerate(tqdm(test_dataloader, position=0, leave=True, desc=\"testing\")):\n","    # forward\n","    test_images, test_labels = test_images.to(device), test_labels.to(device)\n","    test_outputs = loaded_model(test_images)\n","    _, test_preds = torch.max(test_outputs, 1)\n","\n","    final_outs = softmax(test_outputs.detach().cpu().numpy(), axis=1)\n","    test_outputs_list.extend(final_outs)\n","    test_preds_list.extend(test_preds.detach().cpu().numpy())\n","    test_labels_list.extend(test_labels.detach().cpu().numpy())\n","\n","test_preds_list = np.array(test_preds_list)\n","test_labels_list = np.array(test_labels_list)\n","\n","print(f\"\\nacc: {np.mean(test_preds_list == test_labels_list)*100}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M0a4PPefaRfH"},"outputs":[],"source":["# ROC Curve\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score\n","\n","fpr = {}\n","tpr = {}\n","thresh = {}\n","n_class = 10\n","\n","for i in range(n_class):\n","    fpr[i], tpr[i], thresh[i] = roc_curve(test_labels_list, np.array(test_outputs_list)[:, i], pos_label=i)\n","\n","# plot.\n","for i in range(n_class):\n","    plt.plot(fpr[i], tpr[i], linestyle=\"--\", label=f\"Class {i} vs Rest\")\n","plt.title(\"Multi-class ROC Curve\")\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.legend(loc=\"best\")\n","plt.show()\n","\n","print(\"auc_score\", roc_auc_score(test_labels_list, test_outputs_list, multi_class=\"ovo\", average=\"macro\"))"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNV8I4xg9IlpnHojmJVgHse","collapsed_sections":[],"mount_file_id":"1gNh-HsjMOM_aMXda7EfQdjZjqAA3DNqt","name":"05-2. Efficient Net Fine-tune","private_outputs":true,"provenance":[{"file_id":"1Z45RhZD6DFfZgDsoSGZhYKoAhgf4szLf","timestamp":1628427684925},{"file_id":"1vOwqGgv4OKk-1vYWcH8uFpvoIooaUeTj","timestamp":1628424146619},{"file_id":"1n37-PsBNU1tEXNwUvWohvHVJD81-C8m2","timestamp":1627212638626},{"file_id":"1gNh-HsjMOM_aMXda7EfQdjZjqAA3DNqt","timestamp":1626696451891}]},"kernelspec":{"display_name":"jupyter_notebook","language":"python","name":"deeplearning"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.16"}},"nbformat":4,"nbformat_minor":0}
